{
  "cells": [
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "from tensorforce.environments import Environment\nfrom tensorforce.agents import Agent\nimport tensorforce",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "class ReinforcementLearning:\n        \n    def __init__(self, ReinfocementLearning_Agent, Training_Eviroment, Demonstration_Eviroment) :\n            \n        # Reinforcement Agent Policy\n        self.I_Am_ReinfocementLearning_Agent = ReinfocementLearning_Agent\n        \n        # Reinforcement Learning Eviroment    \n        self.I_Am_Training_Eviroment = Training_Eviroment\n        self.I_Am_Demonstration_Eviroment = Demonstration_Eviroment  \n        \n        # Traning Parameter \n        self.TrainingSampleTimes = 0\n        self.TrainingRecord_Sampled_Trajectory = []\n        self.TrainingRecord_Accumulated_Rewards = []\n        self.TrainingRecord_Terminate_Steps = []\n        \n        # Demonstration Parameter\n        self.DemonstrationSampleTimes = 0\n        self.DemonstrationRecord_Accumulated_Rewards = []\n        self.DemonstrationRecord_Terminate_Steps = []\n \n    \n    def AddRewardFunction(self, ObjectFunction_CaulculateReward):        \n        self.I_Am_Reward_Function = ObjectFunction_CaulculateReward.I_Am_Reward_Function\n    \n    def Training_Agent(self, TrainingSampleTimes):\n        \n        self.TrainingSampleTimes = TrainingSampleTimes\n        environment = self.I_Am_Training_Eviroment\n        agent = self.I_Am_ReinfocementLearning_Agent\n\n        for Sampling_Trajectory in range(self.TrainingSampleTimes):\n\n            states = environment.reset()\n            terminal = False\n            \n            Current_Steps = 0\n            Final_Steps = Current_Steps\n            Current_Trajectory_Record = []\n            sum_rewards = 0.0\n            \n            print(\"========================================================================\")\n            print(\"Training Trajectory : {}\".format(Sampling_Trajectory))\n            print(\"========================================================================\")\n            \n            while not terminal:          \n                actions = agent.act(states=states)\n                states, terminal, reward = environment.execute(actions=actions)\n                reward = self.I_Am_Reward_Function(states, reward)\n                agent.observe(terminal=terminal, reward=reward)\n\n                print(\"Step:{}\".format(Current_Steps))\n                print(\"Action: {} Reward: {} States: {} \".format(actions, reward, states))\n\n                Current_Steps = Current_Steps + 1   \n                Current_Trajectory_Record.append({\"Action\": actions, \"Reward\": reward, \"States\": states})\n                Final_Steps = Current_Steps\n                sum_rewards = sum_rewards + reward\n                \n            print('Empirical Total Reward:', sum_rewards)\n            self.TrainingRecord_Terminate_Steps.append(Final_Steps)\n            self.TrainingRecord_Accumulated_Rewards.append(sum_rewards)\n            self.TrainingRecord_Sampled_Trajectory.append(Current_Trajectory_Record)\n            \n        environment.close()\n        \n    def Demonstrating_Agent(self, DemonstrationSampleTimes):\n        \n        self.DemonstrationSampleTimes = DemonstrationSampleTimes\n        environment = self.I_Am_Demonstration_Eviroment\n        agent = self.I_Am_ReinfocementLearning_Agent\n        \n        for Sampling_Trajectory in range(self.DemonstrationSampleTimes):\n            \n            states = environment.reset()\n            internals = agent.initial_internals()\n            terminal = False\n            \n            Current_Steps = 0\n            Final_Steps = Current_Steps\n            sum_rewards = 0.0\n\n            print(\"========================================================================\")\n            print(\"Demonstration Trajectory : {}\".format(Sampling_Trajectory))\n            print(\"========================================================================\")            \n            \n            while not terminal:\n                \n                actions, internals = agent.act(states=states, internals=internals, evaluation=True)\n                states, terminal, reward = environment.execute(actions=actions)\n                reward = self.I_Am_Reward_Function(states, reward)\n                \n                print(\"Step:{}\".format(Current_Steps))\n                print(\"Action: {} Reward: {} States: {} \".format(actions, reward, states))\n                \n                Current_Steps = Current_Steps + 1\n                Final_Steps = Current_Steps\n                sum_rewards = sum_rewards + reward\n                \n            print('Empirical Total Reward:', sum_rewards)\n            self.DemonstrationRecord_Terminate_Steps.append(Final_Steps)         \n            self.DemonstrationRecord_Accumulated_Rewards.append(sum_rewards)\n            \n        environment.close()\n        \n    def Showing_Statistic_Graphics(self):\n        \n        print(\"Showing Graphical Pannel like matalib\")\n        print(self.TrainingRecord_Sampled_Trajectory)\n        print(self.TrainingRecord_Accumulated_Rewards)\n        print(self.TrainingRecord_Terminate_Steps)\n        print(self.DemonstrationRecord_Accumulated_Rewards)\n        print(self.DemonstrationRecord_Terminate_Steps)        \n",
      "execution_count": 2,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# GYM\n\n# Discrete\n# Acrobot-v1 # CartPole-v1\n\n# Countinous\n# MountainCarContinuous-v0 # Pendulum-v0",
      "execution_count": 12,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Discrete",
      "execution_count": 13,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Discrete\n# CartPole-v1\n\n# Objective Reward Function\nclass ObjectFunction_RewardCaulculation_CartPole:\n    \n    def I_Am_Reward_Function(states, reward):\n        x, x_dot, theta, theta_dot = states\n        reward_Position =  (10*(1 - abs(x)))**3\n        reward_Angle =  (100*(0.1 - abs(theta)))**3\n        I_Am_Official_Reward = reward_Position + reward_Angle  \n        \n        return I_Am_Official_Reward",
      "execution_count": 14,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Dynamic Programing",
      "execution_count": 79,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Deep Q-Network\n\nTraining_Environment = Environment.create(environment='gym', level='CartPole-v1', max_episode_timesteps=500,  visualize=False)\nDemo_Environment = Environment.create(environment='gym', level='CartPole-v1', max_episode_timesteps=500,  visualize=True)\n\nlayer_config = [\n    dict(type='dense', size=4, activation='relu'),\n    dict(type='dense', size=128, activation='relu'),\n    dict(type='dense', size=64, activation='relu'),\n    dict(type='dense', size=128, activation='relu'),\n    dict(type='dense', size=64, activation='relu'),\n    dict(type='dense', size=128, activation='relu'),\n    dict(type='flatten')]\n\nagent_DQN = Agent.create(agent=\"dqn\", network = layer_config, environment=Training_Environment, learning_rate=1e-2 , memory = 1000)    \n    \nReinforcementLearning_Instance_CartPole_DQN = ReinforcementLearning(agent_DQN, Training_Environment, Demo_Environment)\nReinforcementLearning_Instance_CartPole_DQN.AddRewardFunction(ObjectFunction_RewardCaulculation_CartPole)\n\nReinforcementLearning_Instance_CartPole_DQN.Training_Agent(300)\nReinforcementLearning_Instance_CartPole_DQN.Demonstrating_Agent(10)\nprint(ReinforcementLearning_Instance_CartPole_DQN.TrainingRecord_Accumulated_Rewards)\nprint(ReinforcementLearning_Instance_CartPole_DQN.DemonstrationRecord_Accumulated_Rewards)\nagent_DQN.close()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Policy Graduim",
      "execution_count": 16,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Proximal Policy Optimization\n\nTraining_Environment = Environment.create(environment='gym', level='CartPole-v1', max_episode_timesteps=500,  visualize=False)\nDemo_Environment = Environment.create(environment='gym', level='CartPole-v1', max_episode_timesteps=500,  visualize=True)\n\nlayer_config = [\n    dict(type='dense', size=4, activation='relu'),\n    dict(type='dense', size=128, activation='relu'),\n    dict(type='dense', size=64, activation='relu'),\n    dict(type='dense', size=128, activation='relu'),\n    dict(type='dense', size=64, activation='relu'),\n    dict(type='dense', size=128, activation='relu'),\n    dict(type='flatten')]\n\nagent_PPO = Agent.create(agent=\"ppo\", network = layer_config, environment=Training_Environment, batch_size=10, learning_rate=1e-3)    \n    \nReinforcementLearning_Instance_CartPole_PPO = ReinforcementLearning(agent_PPO, Training_Environment, Demo_Environment)\nReinforcementLearning_Instance_CartPole_PPO.AddRewardFunction(ObjectFunction_RewardCaulculation_CartPole)\n\nReinforcementLearning_Instance_CartPole_PPO.Training_Agent(300)\nReinforcementLearning_Instance_CartPole_PPO.Demonstrating_Agent(10)\nprint(ReinforcementLearning_Instance_CartPole_PPO.TrainingRecord_Accumulated_Rewards)\nprint(ReinforcementLearning_Instance_CartPole_PPO.DemonstrationRecord_Accumulated_Rewards)\nagent_PPO.close()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Actor-Critic",
      "execution_count": 18,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Adventage Actor Critic\n\nTraining_Environment = Environment.create(environment='gym', level='CartPole-v1', max_episode_timesteps=500,  visualize=False)\nDemo_Environment = Environment.create(environment='gym', level='CartPole-v1', max_episode_timesteps=500,  visualize=True)\n\nlayer_config = [\n    dict(type='dense', size=4, activation='relu'),\n    dict(type='dense', size=128, activation='relu'),\n    dict(type='dense', size=64, activation='relu'),\n    dict(type='dense', size=128, activation='relu'),\n    dict(type='dense', size=64, activation='relu'),\n    dict(type='dense', size=128, activation='relu'),\n    dict(type='flatten')]\n\nagent_A2C = Agent.create(agent=\"a2c\", network = layer_config, environment=Training_Environment, learning_rate=1e-3)\n   \nReinforcementLearning_Instance_CartPole_A2C = ReinforcementLearning(agent_A2C, Training_Environment, Demo_Environment)\nReinforcementLearning_Instance_CartPole_A2C.AddRewardFunction(ObjectFunction_RewardCaulculation_CartPole)\n\nReinforcementLearning_Instance_CartPole_A2C.Training_Agent(300)\nReinforcementLearning_Instance_CartPole_A2C.Demonstrating_Agent(10)\nprint(ReinforcementLearning_Instance_CartPole_A2C.TrainingRecord_Accumulated_Rewards)\nprint(ReinforcementLearning_Instance_CartPole_A2C.DemonstrationRecord_Accumulated_Rewards)\nagent_A2C.close()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Discrete",
      "execution_count": 20,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Discrete\n# Acrobot-v1\n\n# Objective Reward Function\nclass ObjectFunction_RewardCaulculation_Acrobot:\n    \n    def I_Am_Reward_Function(states, reward):\n        \n        I_Am_Official_Reward = -100 + abs(10*states[4]) + abs(10*states[5])**3\n        \n        return I_Am_Official_Reward",
      "execution_count": 21,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Dynamic Programing",
      "execution_count": 22,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Deep Q-Network\nTraining_Environment = Environment.create(environment='gym', level='Acrobot-v1', max_episode_timesteps=500,  visualize=False)\nDemo_Environment = Environment.create(environment='gym', level='Acrobot-v1', max_episode_timesteps=500,  visualize=True)\n\nlayer_config = [\n    dict(type='dense', size=6, activation='relu'),\n    dict(type='dense', size=128, activation='relu'),\n    dict(type='dense', size=64, activation='relu'),\n    dict(type='dense', size=128, activation='relu'),\n    dict(type='dense', size=64, activation='relu'),\n    dict(type='dense', size=128, activation='relu'),\n    dict(type='flatten')]\nagent_DQN = Agent.create(agent=\"dqn\", network= layer_config, environment=Training_Environment, learning_rate=1e-2 , memory = 1000)    \n\n#agent_DQN = tensorforce.agents.DeepQNetwork(states = Training_Environment.states(), actions = Training_Environment.actions(), max_episode_timesteps = 500,network = layer_config, memory = 1000 , learning_rate=1e-3, horizon=0, discount=0.99)\n#test_network = tensorforce.core.networks.LayeredNetwork( name = \"lstm\",  inputs_spec = {'type': 'float', 'shape': (4,)}, layers = layer_config)\n\n\n\nReinforcementLearning_Instance_Acrobot_DQN = ReinforcementLearning(agent_DQN, Training_Environment, Demo_Environment)\nReinforcementLearning_Instance_Acrobot_DQN.AddRewardFunction(ObjectFunction_RewardCaulculation_Acrobot)\n\nReinforcementLearning_Instance_Acrobot_DQN.Training_Agent(300)\nReinforcementLearning_Instance_Acrobot_DQN.Demonstrating_Agent(10)\nprint(ReinforcementLearning_Instance_Acrobot_DQN.TrainingRecord_Accumulated_Rewards)\nprint(ReinforcementLearning_Instance_Acrobot_DQN.DemonstrationRecord_Accumulated_Rewards)\nagent_DQN.close()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Policy Graduim",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Proximal Policy Optimization\nTraining_Environment = Environment.create(environment='gym', level='Acrobot-v1', max_episode_timesteps=500,  visualize=False)\nDemo_Environment = Environment.create(environment='gym', level='Acrobot-v1', max_episode_timesteps=500,  visualize=True)\n\nlayer_config = [\n    dict(type='dense', size=6, activation='relu'),\n    dict(type='dense', size=128, activation='relu'),\n    dict(type='dense', size=64, activation='relu'),\n    dict(type='dense', size=128, activation='relu'),\n    dict(type='dense', size=64, activation='relu'),\n    dict(type='dense', size=128, activation='relu'),\n    dict(type='flatten')]\nagent_PPO = Agent.create(agent=\"ppo\", network = layer_config, environment=Training_Environment, batch_size=10, learning_rate=1e-3)    \n    \nReinforcementLearning_Instance_Acrobot_PPO = ReinforcementLearning(agent_PPO, Training_Environment, Demo_Environment)\nReinforcementLearning_Instance_Acrobot_PPO.AddRewardFunction(ObjectFunction_RewardCaulculation_Acrobot)\n\nReinforcementLearning_Instance_Acrobot_PPO.Training_Agent(300)\nReinforcementLearning_Instance_Acrobot_PPO.Demonstrating_Agent(10)\nprint(ReinforcementLearning_Instance_Acrobot_PPO.TrainingRecord_Accumulated_Rewards)\nprint(ReinforcementLearning_Instance_Acrobot_PPO.DemonstrationRecord_Accumulated_Rewards)\nagent_PPO.close()",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Step:259\nAction: 0 Reward: -85.79585152397335 States: [ 0.16011905  0.98709771 -0.64877126 -0.76098347 -0.88637214 -0.17479352] \nStep:260\nAction: 1 Reward: 3459.918894749884 States: [ 0.44549804  0.89528291 -0.54372272 -0.83926492 -2.09145858  1.52391367] \nStep:261\nAction: 2 Reward: 52050.49407148783 States: [ 0.83319202  0.55298378 -0.05469583 -0.99850306 -3.08209802  3.7353723 ] \nStep:262\nAction: 1 Reward: 190235.0444711218 States: [ 0.99587086 -0.09078126  0.78635724 -0.61777204 -3.60529366  5.75191106] \nStep:263\nAction: 1 Reward: 170880.01942870262 States: [ 0.70684664 -0.70736683  0.86983662  0.49333989 -3.12010369  5.54994528] \nStep:264\nAction: 1 Reward: 45564.13627155438 States: [ 0.28846927 -0.95748915  0.14035904  0.99010067 -1.76453773  3.57384571] \nStep:265\nAction: 2 Reward: 9660.19407167068 States: [ 0.0720633  -0.99740006 -0.41323518  0.91062434 -0.43798061  2.13675391] \nStep:266\nAction: 2 Reward: 386.7091646316716 States: [ 0.11631976 -0.99321182 -0.65868666  0.75241736  0.8743939   0.78186559] \nStep:267\nAction: 1 Reward: 747.3001829048297 States: [ 0.4089232  -0.9125688  -0.64950498  0.76035733  2.13794057 -0.93823752] \nStep:268\nAction: 1 Reward: 23838.861836101663 States: [ 0.81280148 -0.58254077 -0.32261209  0.94653127  3.0603811  -2.88081904] \nStep:269\nAction: 1 Reward: 116134.8310659693 States: [ 0.99922879  0.03926603  0.43619898  0.89985024  3.47319363 -4.87980161] \nStep:270\nAction: 1 Reward: 187275.86852001122 States: [ 0.7464202   0.66547493  0.99979843  0.02007748  3.25187031 -5.72197682] \nStep:271\nAction: 1 Reward: 72931.57624375421 States: [ 0.30321662  0.95292165  0.54838636 -0.83622509  1.98871895 -4.17956226] \nStep:272\nAction: 1 Reward: 15382.462532434673 States: [ 0.05820583  0.9983046  -0.08186314 -0.99664358  0.50848767 -2.49210188] \nStep:273\nAction: 2 Reward: 249.6417363377996 States: [ 0.11017399  0.99391232 -0.39120424 -0.92030388 -1.01512101 -0.69760441] \nStep:274\nAction: 1 Reward: 1268.7757156938853 States: [ 0.43300397  0.90139202 -0.35878524 -0.93342014 -2.32227978  1.10399454] \nStep:275\nAction: 2 Reward: 42259.65974419157 States: [ 0.85372798  0.52071924  0.0866207  -0.99624136 -3.35718282  3.48499942] \nStep:276\nAction: 1 Reward: 162934.3940537706 States: [ 0.98421465 -0.17697886  0.84067627 -0.54153801 -3.79150339  5.46251624] \nStep:277\nAction: 1 Reward: 129172.84992622073 States: [ 0.62621035 -0.77965415  0.86426961  0.50302888 -3.16863655  5.05592108] \nStep:278\nAction: 1 Reward: 32029.848702616477 States: [ 0.17370343 -0.98479801  0.21875241  0.9757804  -1.82268644  3.17848925] \nStep:279\nAction: 2 Reward: 6578.546174941583 States: [-0.06075183 -0.9981529  -0.27736995  0.96076319 -0.5287225   1.88269221] \nStep:280\nAction: 1 Reward: 1.3381060429769462 States: [-0.02920167 -0.99957354 -0.49341762  0.86979253  0.84025912  0.45296075] \nStep:281\nAction: 1 Reward: 1720.999697757473 States: [ 0.26836996 -0.96331592 -0.43116327  0.90227392  2.14861381 -1.21633081] \nStep:282\nAction: 1 Reward: 36213.41979430144 States: [ 7.27639387e-01 -6.85959855e-01 -2.69871366e-04  9.99999964e-01\n  3.22810633e+00 -3.31050034e+00] \nStep:283\nAction: 1 Reward: 167833.5657177138 States: [ 0.99940137 -0.03459639  0.77866232  0.62744322  3.89577749 -5.51669427] \nStep:284\nAction: 1 Reward: 164145.5849791584 States: [ 0.74465966  0.66744437  0.88900519 -0.45789711  3.52847513 -5.47604235] \nStep:285\nAction: 1 Reward: 41134.41335609891 States: [ 0.25586144  0.96671347  0.19794599 -0.98021293  2.23922694 -3.45415088] \nStep:286\nAction: 1 Reward: 5953.38145082979 States: [-0.05451281  0.99851307 -0.31510456 -0.94905696  0.89658434 -1.82159338] \nStep:287\nAction: 2 Reward: -84.04084365791799 States: [-0.09360209  0.99560969 -0.50045015 -0.86576535 -0.49992211 -0.22212767] \nStep:288\nAction: 1 Reward: 2135.480766954489 States: [ 0.13615866  0.99068704 -0.40882601 -0.91261234 -1.79536304  1.3040362 ] \nStep:289\n"
        }
      ]
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Actor-Critic",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Adventage Actor Critic\n\nTraining_Environment = Environment.create(environment='gym', level='Acrobot-v1', max_episode_timesteps=500,  visualize=False)\nDemo_Environment = Environment.create(environment='gym', level='Acrobot-v1', max_episode_timesteps=500,  visualize=True)\n\nlayer_config = [\n    dict(type='dense', size=6, activation='relu'),\n    dict(type='dense', size=128, activation='relu'),\n    dict(type='dense', size=64, activation='relu'),\n    dict(type='dense', size=128, activation='relu'),\n    dict(type='dense', size=64, activation='relu'),\n    dict(type='dense', size=128, activation='relu'),\n    dict(type='flatten')]\n\nagent_A2C = Agent.create(agent=\"a2c\", network = layer_config, environment=Training_Environment, learning_rate=1e-2)    \n    \nReinforcementLearning_Instance_Acrobot_A2C = ReinforcementLearning(agent_A2C, Training_Environment, Demo_Environment)\nReinforcementLearning_Instance_Acrobot_A2C.AddRewardFunction(ObjectFunction_RewardCaulculation_Acrobot)\n\nReinforcementLearning_Instance_Acrobot_A2C.Training_Agent(300)\nReinforcementLearning_Instance_Acrobot_A2C.Demonstrating_Agent(10)\nprint(ReinforcementLearning_Instance_Acrobot_A2C.TrainingRecord_Accumulated_Rewards)\nprint(ReinforcementLearning_Instance_Acrobot_A2C.DemonstrationRecord_Accumulated_Rewards)\nagent_A2C.close()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Countinous",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Countinous\n# MountainCarContinuous-v0\n\n# Objective Reward Function\nclass ObjectFunction_RewardCaulculation_MountainCarContinuous:\n    \n    def I_Am_Reward_Function(states, reward):\n\n        I_Am_Official_Reward = reward \n        \n        return I_Am_Official_Reward\n",
      "execution_count": 3,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Policy Graduim",
      "execution_count": 4,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Proximal Policy Optimization\n\nTraining_Environment = Environment.create(environment='gym', level='MountainCarContinuous-v0', max_episode_timesteps=500,  visualize=False)\nDemo_Environment = Environment.create(environment='gym', level='MountainCarContinuous-v0', max_episode_timesteps=500,  visualize=True)\nagent_PPO = Agent.create(agent=\"ppo\", environment=Training_Environment, batch_size=10, learning_rate=1e-3)    \n    \nReinforcementLearning_Instance_MountainCarContinuous_PPO = ReinforcementLearning(agent_PPO, Training_Environment, Demo_Environment)\nReinforcementLearning_Instance_MountainCarContinuous_PPO.AddRewardFunction(ObjectFunction_RewardCaulculation_MountainCarContinuous)\n\nReinforcementLearning_Instance_MountainCarContinuous_PPO.Training_Agent(300)\nReinforcementLearning_Instance_MountainCarContinuous_PPO.Demonstrating_Agent(10)\nprint(ReinforcementLearning_Instance_MountainCarContinuous_PPO.TrainingRecord_Accumulated_Rewards)\nprint(ReinforcementLearning_Instance_MountainCarContinuous_PPO.DemonstrationRecord_Accumulated_Rewards)\nagent_PPO.close()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Trust-Region Policy Optimization\n\nTraining_Environment = Environment.create(environment='gym', level='MountainCarContinuous-v0', max_episode_timesteps=500,  visualize=False)\nDemo_Environment = Environment.create(envirotnment='gym', level='MountainCarContinuous-v0', max_episode_timesteps=500,  visualize=True)\nagent_TRPO = Agent.create(agent=\"trpo\", environment=Training_Environment, batch_size=10, learning_rate=1e-2)    \n    \nReinforcementLearning_Instance_MountainCarContinuous_TRPO = ReinforcementLearning(agent_TRPO, Training_Environment, Demo_Environment)\nReinforcementLearning_Instance_MountainCarContinuous_TRPO.AddRewardFunction(ObjectFunction_RewardCaulculation_MountainCarContinuous)\n\nReinforcementLearning_Instance_MountainCarContinuous_TRPO.Training_Agent(300)\nReinforcementLearning_Instance_MountainCarContinuous_TRPO.Demonstrating_Agent(10)\nprint(ReinforcementLearning_Instance_MountainCarContinuous_TRPO.TrainingRecord_Accumulated_Rewards)\nprint(ReinforcementLearning_Instance_MountainCarContinuous_TRPO.DemonstrationRecord_Accumulated_Rewards)\nagent_TRPO.close()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Actor-Critic",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "#  Adventage Actor Critic\n\nTraining_Environment = Environment.create(environment='gym', level='MountainCarContinuous-v0', max_episode_timesteps=500,  visualize=False)\nDemo_Environment = Environment.create(environment='gym', level='MountainCarContinuous-v0', max_episode_timesteps=500,  visualize=True)\n\nlayer_config = [\n    dict(type='dense', size=2, activation='relu'),\n    dict(type='dense', size=128, activation='relu'),\n    dict(type='dense', size=64, activation='relu'),\n    dict(type='dense', size=128, activation='relu'),\n    dict(type='dense', size=64, activation='relu'),\n    dict(type='dense', size=128, activation='relu'),\n    dict(type='dense', size=1, activation='relu')]\n\nagent_A2C = Agent.create(agent=\"a2c\", network = layer_config, environment=Training_Environment, learning_rate=1e-2)    \n    \nReinforcementLearning_Instance_MountainCarContinuous_A2C = ReinforcementLearning(agent_A2C, Training_Environment, Demo_Environment)\nReinforcementLearning_Instance_MountainCarContinuous_A2C.AddRewardFunction(ObjectFunction_RewardCaulculation_MountainCarContinuous)\n\nReinforcementLearning_Instance_MountainCarContinuous_A2C.Training_Agent(300)\nReinforcementLearning_Instance_MountainCarContinuous_A2C.Demonstrating_Agent(10)\nprint(ReinforcementLearning_Instance_MountainCarContinuous_A2C.TrainingRecord_Accumulated_Rewards)\nprint(ReinforcementLearning_Instance_MountainCarContinuous_A2C.DemonstrationRecord_Accumulated_Rewards)\nagent_A2C.close()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Countinous",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Countinous\n# Pendulum-v0\n\n# Objective Reward Function\nclass ObjectFunction_RewardCaulculation_Pendulum:\n    \n    def I_Am_Reward_Function(states, reward):\n\n        I_Am_Official_Reward = reward \n        \n        return I_Am_Official_Reward",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Policy Graduim",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Proximal Policy Optimization\n\nTraining_Environment = Environment.create(environment='gym', level='Pendulum-v0', max_episode_timesteps=200,  visualize=False)\nDemo_Environment = Environment.create(environment='gym', level='Pendulum-v0', max_episode_timesteps=200,  visualize=True)\nagent_PPO = Agent.create(agent=\"ppo\", environment=Training_Environment, batch_size=10, learning_rate=1e-3)    \n    \nReinforcementLearning_Instance_Pendulum_PPO = ReinforcementLearning(agent_PPO, Training_Environment, Demo_Environment)\nReinforcementLearning_Instance_Pendulum_PPO.AddRewardFunction(ObjectFunction_RewardCaulculation_Pendulum)\n\nReinforcementLearning_Instance_Pendulum_PPO.Training_Agent(300)\nReinforcementLearning_Instance_Pendulum_PPO.Demonstrating_Agent(10)\nprint(ReinforcementLearning_Instance_Pendulum_PPO.TrainingRecord_Accumulated_Rewards)\nprint(ReinforcementLearning_Instance_Pendulum_PPO.DemonstrationRecord_Accumulated_Rewards)\nagent_PPO.close()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Trust-Region Policy Optimization\n\nTraining_Environment = Environment.create(environment='gym', level='Pendulum-v0', max_episode_timesteps=200,  visualize=False)\nDemo_Environment = Environment.create(environment='gym', level='Pendulum-v0', max_episode_timesteps=200,  visualize=True)\nagent_TRPO = Agent.create(agent=\"trpo\", environment=Training_Environment, batch_size=10, learning_rate=1e-2)    \n    \nReinforcementLearning_Instance_Pendulum_TRPO = ReinforcementLearning(agent_TRPO, Training_Environment, Demo_Environment)\nReinforcementLearning_Instance_Pendulum_TRPO.AddRewardFunction(ObjectFunction_RewardCaulculation_Pendulum)\n\nReinforcementLearning_Instance_Pendulum_TRPO.Training_Agent(300)\nReinforcementLearning_Instance_Pendulum_TRPO.Demonstrating_Agent(10)\nprint(ReinforcementLearning_Instance_Pendulum_TRPO.TrainingRecord_Accumulated_Rewards)\nprint(ReinforcementLearning_Instance_Pendulum_TRPO.DemonstrationRecord_Accumulated_Rewards)\nagent_TRPO.close()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Actor-Critic",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "#  Adventage Actor Critic\n\nTraining_Environment = Environment.create(environment='gym', level='Pendulum-v0', max_episode_timesteps=200,  visualize=False)\nDemo_Environment = Environment.create(environment='gym', level='Pendulum-v0', max_episode_timesteps=200,  visualize=True)\n\nlayer_config = [\n    dict(type='dense', size=2, activation='relu'),\n    dict(type='dense', size=128, activation='relu'),\n    dict(type='dense', size=64, activation='relu'),\n    dict(type='dense', size=128, activation='relu'),\n    dict(type='dense', size=64, activation='relu'),\n    dict(type='dense', size=128, activation='relu'),\n    dict(type='dense', size=1, activation='relu')]\n\nagent_A2C = Agent.create(agent=\"a2c\", network = layer_config, environment=Training_Environment, learning_rate=1e-2)    \n    \nReinforcementLearning_Instance_Pendulum_A2C = ReinforcementLearning(agent_A2C, Training_Environment, Demo_Environment)\nReinforcementLearning_Instance_Pendulum_A2C.AddRewardFunction(ObjectFunction_RewardCaulculation_Pendulum)\n\nReinforcementLearning_Instance_Pendulum_A2C.Training_Agent(300)\nReinforcementLearning_Instance_Pendulum_A2C.Demonstrating_Agent(10)\nprint(ReinforcementLearning_Instance_Pendulum_A2C.TrainingRecord_Accumulated_Rewards)\nprint(ReinforcementLearning_Instance_Pendulum_A2C.DemonstrationRecord_Accumulated_Rewards)\nagent_A2C.close()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# openai_retro \n\n#the 128 sine-dot by Anthrox\n#Sega Tween by Ben Ryves\n#Happy 10! by Blind IO\n#512-Colour Test Demo by Chris Covell\n#Dekadrive by Dekadence\n#Automaton by Derek Ledbetter\n#Fire by dox\n#FamiCON intro by dr88\n#Airstriker by Electrokinesis\n#Lost Marbles by Vantage",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "class ObjectFunction_RewardCaulculation_openai_retro:\n    \n    def I_Am_Reward_Function(states, reward):\n\n        I_Am_Official_Reward = reward \n        \n        return I_Am_Official_Reward",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Dynamic Programing",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Deep Q-Network\n\nTraining_Environment = Environment.create(environment='openai_retro', level='Airstriker-Genesis', max_episode_timesteps=500,  visualize=False)\nagent_DQN = Agent.create(agent=\"dqn\", environment=Training_Environment, batch_size=10, learning_rate=1e-3 , memory = 10000)    \n    \nReinforcementLearning_Instance_openai_retro_DQN = ReinforcementLearning(agent_DQN, Training_Environment, Training_Environment)\nReinforcementLearning_Instance_openai_retro_DQN.AddRewardFunction(ObjectFunction_RewardCaulculation_openai_retro)\nReinforcementLearning_Instance_openai_retro_DQN.Training_Agent(500)\nprint(ReinforcementLearning_Instance_HandManipulatePen_DQN.TrainingRecord_Accumulated_Rewards)\n\n\nDemo_Environment = Environment.create(environment='openai_retro', level='Airstriker-Genesis', max_episode_timesteps=500,  visualize=True)\nReinforcementLearning_Instance_openai_retro_DQN = ReinforcementLearning(agent_DQN, Demo_Environment, Demo_Environment)\nReinforcementLearning_Instance_openai_retro_DQN.AddRewardFunction(ObjectFunction_RewardCaulculation_openai_retro)\nReinforcementLearning_Instance_openai_retro_DQN.Demonstrating_Agent(10)\nprint(ReinforcementLearning_Instance_HandManipulatePen_DQN.DemonstrationRecord_Accumulated_Rewards)\n\nagent_DQN.close()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Policy Graduim",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Proximal Policy Optimization\n\nTraining_Environment = Environment.create(environment='openai_retro', level='Airstriker-Genesis', max_episode_timesteps=500,  visualize=False)\nagent_PPO = Agent.create(agent=\"ppo\", environment=Training_Environment, batch_size=10, learning_rate=1e-3)   \n    \nReinforcementLearning_Instance_openai_retro_PPO = ReinforcementLearning(agent_DQN, Training_Environment, Training_Environment)\nReinforcementLearning_Instance_openai_retro_PPO.AddRewardFunction(ObjectFunction_RewardCaulculation_openai_retro)\nReinforcementLearning_Instance_openai_retro_PPO.Training_Agent(3000)\nprint(ReinforcementLearning_Instance_openai_retro_PPO.TrainingRecord_Accumulated_Rewards)\n\nDemo_Environment = Environment.create(environment='openai_retro', level='Airstriker-Genesis', max_episode_timesteps=500,  visualize=True)\nReinforcementLearning_Instance_openai_retro_PPO = ReinforcementLearning(agent_DQN, Demo_Environment, Demo_Environment)\nReinforcementLearning_Instance_openai_retro_PPO.AddRewardFunction(ObjectFunction_RewardCaulculation_openai_retro)\nReinforcementLearning_Instance_openai_retro_PPO.Demonstrating_Agent(10)\nprint(ReinforcementLearning_Instance_openai_retro_PPO.DemonstrationRecord_Accumulated_Rewards)\n\nagent_PPO.close()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Actor Critic",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Adventage Actor Critic\n\nTraining_Environment = Environment.create(environment='openai_retro', level='Airstriker-Genesis', max_episode_timesteps=500,  visualize=False)\nagent_A2C = Agent.create(agent=\"a2c\", environment=Training_Environment, batch_size=10, learning_rate=1e-3)   \n    \nReinforcementLearning_Instance_openai_retro_A2C = ReinforcementLearning(agent_A2C, Training_Environment, Training_Environment)\nReinforcementLearning_Instance_openai_retro_A2C.AddRewardFunction(ObjectFunction_RewardCaulculation_openai_retro)\nReinforcementLearning_Instance_openai_retro_A2C.Training_Agent(3000)\nprint(ReinforcementLearning_Instance_openai_retro_A2C.TrainingRecord_Accumulated_Rewards)\n\nDemo_Environment = Environment.create(environment='openai_retro', level='Airstriker-Genesis', max_episode_timesteps=500,  visualize=True)\nReinforcementLearning_Instance_openai_retro_A2C = ReinforcementLearning(agent_DQN, Demo_Environment, Demo_Environment)\nReinforcementLearning_Instance_openai_retro_A2C.AddRewardFunction(ObjectFunction_RewardCaulculation_openai_retro)\nReinforcementLearning_Instance_openai_retro_A2C.Demonstrating_Agent(10)\nprint(ReinforcementLearning_Instance_openai_retro_A2C.DemonstrationRecord_Accumulated_Rewards)\n\nagent_A2C.close()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}